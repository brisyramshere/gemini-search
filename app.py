import streamlit as st
import re
from src.ai_core import get_ai_response_stream, generate_research_report_stream
from src.utils import save_conversations_to_file, load_conversations_from_file
from src.config import init_config, get_config, update_config, save_config

# --- é¡µé¢é…ç½® ---
st.set_page_config(
    page_title="Gemini è”ç½‘æœç´¢èŠå¤©",
    page_icon="ğŸ¤–",
    layout="wide",
)

# --- åˆå§‹åŒ–é…ç½® ---
init_config()

# --- ä¼šè¯çŠ¶æ€åˆå§‹åŒ– ---
if "conversations" not in st.session_state:
    conversations_data = load_conversations_from_file()
    if conversations_data:
        st.session_state.conversations = conversations_data.get("conversations", {})
        st.session_state.current_conversation_id = conversations_data.get("current_conversation_id", "1")
        st.session_state.next_conversation_id = conversations_data.get("next_conversation_id", 2)
    else:
        st.session_state.conversations = {
            "1": {"title": "æ–°å¯¹è¯ 1", "messages": [{"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘å¯ä»¥è”ç½‘æŸ¥è¯¢æœ€æ–°ä¿¡æ¯ï¼Œå¹¶å‘Šè¯‰ä½ æˆ‘çš„ä¿¡æ¯æ¥æºã€‚", "type": "chat"}]}
        }
        st.session_state.current_conversation_id = "1"
        st.session_state.next_conversation_id = 2
    save_conversations_to_file({
        "conversations": st.session_state.conversations,
        "current_conversation_id": st.session_state.current_conversation_id,
        "next_conversation_id": st.session_state.next_conversation_id
    })

if "generating_report" not in st.session_state:
    st.session_state.generating_report = False

# --- å‡½æ•°å®šä¹‰ ---
def create_new_conversation():
    """åˆ›å»ºä¸€ä¸ªæ–°çš„å¯¹è¯ã€‚"""
    new_id = str(st.session_state.next_conversation_id)
    st.session_state.conversations[new_id] = {
        "title": f"æ–°å¯¹è¯ {new_id}",
        "messages": [{"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘å¯ä»¥è”ç½‘æŸ¥è¯¢æœ€æ–°ä¿¡æ¯ï¼Œå¹¶å‘Šè¯‰ä½ æˆ‘çš„ä¿¡æ¯æ¥æºã€‚", "type": "chat"}]
    }
    st.session_state.current_conversation_id = new_id
    st.session_state.next_conversation_id += 1
    save_conversations_to_file({
        "conversations": st.session_state.conversations,
        "current_conversation_id": st.session_state.current_conversation_id,
        "next_conversation_id": st.session_state.next_conversation_id
    })
    st.rerun()

# --- ä¾§è¾¹æ  ---
with st.sidebar:
    st.header("Gemini æœç´¢")

    # --- å¯¹è¯ç®¡ç†ä¸è®¾ç½® ---
    col1, col2 = st.columns([0.8, 0.2])
    with col1:
        if st.button("â• æ–°å¯¹è¯", use_container_width=True):
            create_new_conversation()
    with col2:
        settings_popover = st.popover("âš™", use_container_width=True)

    with settings_popover:
        with st.form("settings_form"):
            st.subheader("API é…ç½®")
            api_endpoint = st.text_input("API Endpoint", value=get_config("api_endpoint"))
            api_key = st.text_input("Gemini API Key", type="password", value=get_config("api_key"))

            st.subheader("æ¨¡å‹é€‰æ‹©")
            model_options = ["models/gemini-2.5-pro", "models/gemini-2.5-flash", "models/gemini-2.0-flash-001"]
            search_model = st.selectbox(
                "æœç´¢æ¨¡å‹",
                options=model_options,
                index=model_options.index(get_config("search_model", "models/gemini-2.5-flash"))
            )
            report_model = st.selectbox(
                "æŠ¥å‘Šç”Ÿæˆæ¨¡å‹",
                options=model_options,
                index=model_options.index(get_config("report_model", "models/gemini-2.5-pro"))
            )

            st.subheader("æç¤ºè¯ (Prompts)")
            system_prompt = st.text_area("ç³»ç»Ÿæç¤ºè¯", value=get_config("system_prompt"), height=200)
            report_prompt = st.text_area("æŠ¥å‘Šç”Ÿæˆæç¤ºè¯", value=get_config("report_prompt"), height=200)

            submitted = st.form_submit_button("ä¿å­˜è®¾ç½®")
            if submitted:
                update_config("api_endpoint", api_endpoint)
                update_config("api_key", api_key)
                update_config("search_model", search_model)
                update_config("report_model", report_model)
                update_config("system_prompt", system_prompt)
                update_config("report_prompt", report_prompt)
                st.toast("è®¾ç½®å·²ä¿å­˜ï¼")
                # Popover will close automatically on rerun, no need for .close()
                st.rerun()
    
    st.header("å†å²å¯¹è¯")

    conversation_ids = list(st.session_state.conversations.keys())
    current_id = st.session_state.current_conversation_id
    
    # Ensure current_conversation_id is valid
    if current_id not in conversation_ids:
        current_id = conversation_ids[0] if conversation_ids else "1"
        st.session_state.current_conversation_id = current_id

    # --- ä¾§è¾¹æ å¯¹è¯åˆ—è¡¨ ---
    for conv_id in conversation_ids:
        with st.container():
            col1, col2 = st.columns([0.8, 0.2])
            with col1:
                if st.button(st.session_state.conversations[conv_id]["title"], key=f"conv_select_{conv_id}", use_container_width=True):
                    st.session_state.current_conversation_id = conv_id
                    st.rerun()
            with col2:
                if st.button("ğŸ—‘ï¸", key=f"conv_delete_{conv_id}", use_container_width=True):
                    # åˆ é™¤å¯¹è¯
                    del st.session_state.conversations[conv_id]
                    # å¦‚æœåˆ é™¤çš„æ˜¯å½“å‰å¯¹è¯ï¼Œåˆ™åˆ‡æ¢åˆ°åˆ—è¡¨ä¸­çš„ç¬¬ä¸€ä¸ªå¯¹è¯
                    if st.session_state.current_conversation_id == conv_id:
                        if st.session_state.conversations:
                            st.session_state.current_conversation_id = list(st.session_state.conversations.keys())[0]
                        else:
                            # å¦‚æœæ‰€æœ‰å¯¹è¯éƒ½è¢«åˆ é™¤äº†ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„
                            create_new_conversation()
                    save_conversations_to_file({
                        "conversations": st.session_state.conversations,
                        "current_conversation_id": st.session_state.current_conversation_id,
                        "next_conversation_id": st.session_state.next_conversation_id
                    })
                    st.rerun()

    st.markdown("---")
    st.info("ç‚¹å‡»â€œç”Ÿæˆè°ƒç ”æŠ¥å‘Šâ€æŒ‰é’®ï¼Œå¯å°†å½“å‰å¯¹è¯å†…å®¹åˆæˆä¸ºä¸€ä»½ç ”ç©¶æŠ¥å‘Šã€‚")

# --- è·å–å½“å‰å¯¹è¯ ---
current_conversation = st.session_state.conversations[st.session_state.current_conversation_id]

# --- é¡µé¢æ ‡é¢˜ ---
st.title("ğŸ¤– Gemini è”ç½‘æœç´¢èŠå¤©")
st.header(f"{current_conversation['title']}")
st.caption(f"ä¸€ä¸ªèƒ½å±•ç¤ºæ€è€ƒè¿‡ç¨‹å¹¶å¼•ç”¨æ¥æºçš„AIèŠå¤©æœºå™¨äºº (æœç´¢æ¨¡å‹: {get_config('search_model')})")

# --- æ˜¾ç¤ºå†å²æ¶ˆæ¯ ---
for i, message in enumerate(current_conversation["messages"]):
    with st.chat_message(message["role"]):
        col1, col2 = st.columns([0.9, 0.1])
        with col1:
            st.markdown(message["content"])
            
            # å¦‚æœæ¶ˆæ¯æ˜¯æŠ¥å‘Šå¹¶ä¸”æœ‰å¼•ç”¨ï¼Œåˆ™åœ¨æŠ˜å æ§ä»¶ä¸­æ˜¾ç¤ºå®ƒä»¬
            if message.get("type") == "report" and "references" in message and message["references"]:
                with st.expander("å‚è€ƒæ–‡çŒ®"):
                    st.markdown(message["references"])

            if message["role"] == "assistant" and "model" in message:
                st.caption(f"ç”± {message['model']} ç”Ÿæˆ")
        with col2:
            if st.button("ğŸ—‘ï¸", key=f"msg_delete_{st.session_state.current_conversation_id}_{i}", use_container_width=True):
                del current_conversation["messages"][i]
                save_conversations_to_file({
                    "conversations": st.session_state.conversations,
                    "current_conversation_id": st.session_state.current_conversation_id,
                    "next_conversation_id": st.session_state.next_conversation_id
                })
                st.rerun()


# --- æŠ¥å‘Šç”Ÿæˆå¤„ç† ---
if st.session_state.get("generating_report"):
    st.session_state.generating_report = False
    with st.chat_message("assistant"):
        report_placeholder = st.empty()
        report_model_name = get_config("report_model")
        report_body = ""
        final_references = ""

        try:
            history = current_conversation["messages"]
            response_stream = generate_research_report_stream(history, model_name=report_model_name)
            
            for event in response_stream:
                if event["type"] == "report_chunk":
                    report_body += event["chunk"]
                    report_placeholder.markdown(report_body + "â–Œ")
                elif event["type"] == "final_references":
                    final_references = event["content"]
                elif event["type"] == "error":
                    st.error(event["message"])
                    report_body = event["message"]
                    break
            
            # æ¸…ç†å¯èƒ½ç”±æ¨¡å‹æ„å¤–ç”Ÿæˆçš„å¼•ç”¨æ ‡é¢˜
            cleaned_body = re.sub(r"(?i)\s*(#+\s*References|#+\s*å‚è€ƒæ–‡çŒ®)\s*$", "", report_body.strip())
            
            # åœ¨UIä¸Šæ˜¾ç¤ºæœ€ç»ˆçš„æŠ¥å‘Šæ­£æ–‡
            report_placeholder.markdown(cleaned_body)

            # åœ¨ç‹¬ç«‹çš„æŠ˜å æ§ä»¶ä¸­æ˜¾ç¤ºå¼•ç”¨
            if final_references:
                with st.expander("å‚è€ƒæ–‡çŒ®"):
                    st.markdown(final_references)

        except Exception as e:
            st.error(f"ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºç°ä¸¥é‡é”™è¯¯: {e}")
            cleaned_body = "ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºç°ä¸¥é‡é”™è¯¯ï¼Œè¯·æ£€æŸ¥åå°æ—¥å¿—ã€‚"
            report_placeholder.markdown(cleaned_body)

    # å°†ç»“æ„åŒ–çš„æŠ¥å‘Šï¼ˆæ­£æ–‡å’Œå¼•ç”¨åˆ†ç¦»ï¼‰ä¿å­˜åˆ°å¯¹è¯å†å²
    if cleaned_body or final_references:
        current_conversation["messages"].append({
            "role": "assistant", 
            "content": cleaned_body, 
            "references": final_references,
            "type": "report", 
            "model": report_model_name
        })
        save_conversations_to_file({
            "conversations": st.session_state.conversations,
            "current_conversation_id": st.session_state.current_conversation_id,
            "next_conversation_id": st.session_state.next_conversation_id
        })
        st.rerun()

# --- åº•éƒ¨åŠŸèƒ½æŒ‰é’® ---
st.markdown("---")
col1, col2 = st.columns(2)
with col1:
    if st.button("ğŸ“ ç”Ÿæˆè°ƒç ”æŠ¥å‘Š", use_container_width=True):
        # æ£€æŸ¥ API Key æ˜¯å¦å·²è®¾ç½®
        if not get_config("api_key"):
            st.error("è¯·å…ˆåœ¨ä¾§è¾¹æ çš„â€œè®¾ç½®â€ä¸­è¾“å…¥æ‚¨çš„ Gemini API Keyã€‚")
        else:
            st.session_state.generating_report = True
            st.rerun()
with col2:
    # å‡†å¤‡ä¸‹è½½å†…å®¹
    full_conversation_md = f"# {current_conversation['title']}\n\n"
    for msg in current_conversation["messages"]:
        full_conversation_md += f"**{msg['role'].capitalize()}**: \n\n{msg['content']}\n\n---\n\n"
    
    report_title_for_file = re.sub(r'[\\/*?_<>|:]','_', current_conversation['title'])
    st.download_button(
        label="ğŸ“¥ ä¿å­˜ä¸º Markdown",
        data=full_conversation_md,
        file_name=f"å¯¹è¯è®°å½• - {report_title_for_file}.md",
        mime="text/markdown",
        use_container_width=True,
    )

# --- èŠå¤©è¾“å…¥æ¡† ---
if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
    # æ£€æŸ¥ API Key æ˜¯å¦å·²è®¾ç½®
    if not get_config("api_key"):
        st.error("è¯·å…ˆåœ¨ä¾§è¾¹æ çš„â€œè®¾ç½®â€ä¸­è¾“å…¥æ‚¨çš„ Gemini API Keyã€‚")
    else:
        current_conversation["messages"].append({"role": "user", "content": prompt, "type": "chat"})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            full_response = ""
            response_placeholder = st.empty()
            search_model_name = get_config("search_model") # è·å–æ¨¡å‹åç§°
            
            history = current_conversation["messages"][:-1]

            try:
                # ä½¿ç”¨é…ç½®ä¸­çš„æœç´¢æ¨¡å‹
                response_stream = get_ai_response_stream(prompt, model_name=search_model_name, history=history)
                
                for event in response_stream:
                    if event["type"] == "tool_code":
                        st.info(f"ğŸ” æ­£åœ¨æœç´¢: `{event['query']}`")
                    elif event["type"] == "text_chunk":
                        full_response += event["chunk"]
                        response_placeholder.markdown(full_response + "â–Œ")
                    elif event["type"] == "final_response":
                        # The stream is complete, finalize the display
                        response_placeholder.markdown(full_response)
                    elif event["type"] == "error":
                        st.error(event["message"])
                        full_response = event["message"]
                        break
                
            except Exception as e:
                st.error(f"åº”ç”¨å‡ºç°ä¸¥é‡é”™è¯¯: {e}")
                full_response = "åº”ç”¨å‡ºç°ä¸¥é‡é”™è¯¯ï¼Œè¯·æ£€æŸ¥åå°æ—¥å¿—ã€‚"

        current_conversation["messages"].append({"role": "assistant", "content": full_response, "type": "chat", "model": search_model_name})

        if len(current_conversation["messages"]) <= 3:
            current_conversation["title"] = prompt[:30] + ("..." if len(prompt) > 30 else "")
        
        save_conversations_to_file({
            "conversations": st.session_state.conversations,
            "current_conversation_id": st.session_state.current_conversation_id,
            "next_conversation_id": st.session_state.next_conversation_id
        })
        st.rerun()

