import streamlit as st
from src.ai_core import get_ai_response_stream
from src.utils import correct_references

# --- é¡µé¢é…ç½® ---
st.set_page_config(
    page_title="Gemini è”ç½‘æœç´¢èŠå¤©",
    page_icon="ğŸ¤–",
    layout="wide",
)

# --- ä¾§è¾¹æ  ---
with st.sidebar:
    st.header("æ¨¡å‹é…ç½®")
    # ä¿®æ­£æ¨¡å‹åç§°ï¼Œä½¿ç”¨å®˜æ–¹è¦æ±‚çš„ "models/" å‰ç¼€
    # ç§»é™¤äº†ä¸å­˜åœ¨çš„å ä½æ¨¡å‹ï¼Œä»¥é¿å… API é”™è¯¯
    model_options = {
        "Gemini 2.5 Flash (æ¨è)": "models/gemini-2.5-flash",
        "Gemini 2.0 Flash": "models/gemini-2.0-flash-001",
    }
    
    # åˆå§‹åŒ–æ¨¡å‹é€‰æ‹©
    if "selected_model" not in st.session_state or st.session_state.selected_model not in model_options.values():
        st.session_state.selected_model = "models/gemini-2.5-flash"

    # è·å–å½“å‰æ¨¡å‹çš„æ˜¾ç¤ºåç§°
    current_model_key = [key for key, value in model_options.items() if value == st.session_state.selected_model][0]

    # åˆ›å»ºæ¨¡å‹é€‰æ‹©ä¸‹æ‹‰æ¡†
    selected_model_key = st.selectbox(
        "é€‰æ‹©ä¸€ä¸ª Gemini æ¨¡å‹:",
        options=list(model_options.keys()),
        index=list(model_options.keys()).index(current_model_key) # æ ¹æ®ä¼šè¯çŠ¶æ€è®¾ç½®é»˜è®¤å€¼
    )
    st.session_state.selected_model = model_options[selected_model_key]

    st.markdown("---")
    st.markdown("è¿™æ˜¯ä¸€ä¸ªä½¿ç”¨ Gemini API æ„å»ºçš„è”ç½‘æœç´¢èŠå¤©æœºå™¨äººã€‚")
    st.markdown("å®ƒèƒ½å¤Ÿç†è§£æ‚¨çš„é—®é¢˜ï¼Œä¸Šç½‘æœç´¢ç›¸å…³ä¿¡æ¯ï¼Œå¹¶æ ¹æ®æœç´¢ç»“æœç”Ÿæˆå›ç­”ï¼ŒåŒæ—¶æä¾›ä¿¡æ¯æ¥æºã€‚")


# --- é¡µé¢æ ‡é¢˜ ---
st.title("ğŸ¤– Gemini è”ç½‘æœç´¢èŠå¤© (Pro)")
st.caption(f"ä¸€ä¸ªèƒ½å±•ç¤ºæ€è€ƒè¿‡ç¨‹å¹¶å¼•ç”¨æ¥æºçš„AIèŠå¤©æœºå™¨äºº (å½“å‰æ¨¡å‹: {selected_model_key})")




# --- åˆå§‹åŒ–èŠå¤©çŠ¶æ€ ---
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘å¯ä»¥è”ç½‘æŸ¥è¯¢æœ€æ–°ä¿¡æ¯ï¼Œå¹¶å‘Šè¯‰ä½ æˆ‘çš„ä¿¡æ¯æ¥æºã€‚"}
    ]

# --- æ˜¾ç¤ºå†å²æ¶ˆæ¯ ---
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- èŠå¤©è¾“å…¥æ¡† ---
if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        full_response = ""
        response_placeholder = st.empty()
        citations_from_api = []
        
        try:
            response_stream = get_ai_response_stream(prompt, model_name=st.session_state.selected_model)
            
            for event in response_stream:
                if event["type"] == "tool_call":
                    st.info(f"ğŸ” æ­£åœ¨æœç´¢: `{event['query']}`")
                
                elif event["type"] == "text_chunk":
                    full_response += event["chunk"]
                    response_placeholder.markdown(full_response + "â–Œ")
                
                elif event["type"] == "final_response":
                    citations_from_api = event.get("citations", [])
                    final_corrected_text = correct_references(full_response, citations_from_api)
                    response_placeholder.markdown(final_corrected_text)
                    full_response = final_corrected_text

                elif event["type"] == "error":
                    st.error(event["message"])
                    full_response = event["message"]
                    break
            
        except Exception as e:
            st.error(f"åº”ç”¨å‡ºç°ä¸¥é‡é”™è¯¯: {e}")
            full_response = "åº”ç”¨å‡ºç°ä¸¥é‡é”™è¯¯ï¼Œè¯·æ£€æŸ¥åå°æ—¥å¿—ã€‚"

    st.session_state.messages.append({"role": "assistant", "content": full_response})