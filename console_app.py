import sys
import os
from rich.console import Console
from rich.markdown import Markdown
from rich.panel import Panel
import asyncio
import re

# å°†srcç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥æ¨¡å—
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), 'src')))

from src.ai_core import get_ai_response_stream, generate_research_report_stream
from src.config import init_config, get_config
from src.utils import preprocess_history_for_report

# --- å…¨å±€å˜é‡ ---
console = Console()
SEARCH_MODEL_FOR_TEST = "models/gemini-2.0-flash-001" 
REPORT_MODEL_FOR_TEST = "models/gemini-2.5-pro"

async def run_chat_turn(prompt, history):
    """è¿è¡Œä¸€è½®èŠå¤©å¯¹è¯å¹¶è¿”å›æœ€ç»ˆç»“æœã€‚"""
    console.print(f"[bold green]You:[/bold green] {prompt}")
    console.print("\n[bold blue]Assistant:[/bold blue] ", end="")
    
    full_response = ""
    
    response_stream = get_ai_response_stream(prompt, model_name=SEARCH_MODEL_FOR_TEST, history=history)
    
    for event in response_stream:
        if event["type"] == "tool_code":
            console.print(f"\n[yellow]ğŸ” æ­£åœ¨æœç´¢: `{event['query']}`[/yellow]")
        elif event["type"] == "text_chunk":
            print(event["chunk"], end="", flush=True)
            full_response += event["chunk"]
        elif event["type"] == "final_response":
            print()
        elif event["type"] == "error":
            console.print(f"\n[bold red]é”™è¯¯:[/bold red] {event['message']}")
            full_response = event["message"]
            break
            
    console.print("-" * 50)
    return {"role": "assistant", "content": full_response, "type": "chat"}

async def run_report_generation(history):
    """è¿è¡ŒæŠ¥å‘Šç”Ÿæˆå¹¶æ‰“å°ç»“æœï¼Œç°åœ¨ä¼šè‡ªå·±é™„åŠ å¼•ç”¨åˆ—è¡¨ã€‚"""
    console.print(Panel("[bold cyan]ğŸš€ å¼€å§‹ç”Ÿæˆç ”ç©¶æŠ¥å‘Š...[/bold cyan]", expand=False))

    # --- è°ƒè¯•æ­¥éª¤ï¼šåœ¨è°ƒç”¨AIä¹‹å‰ï¼Œå…ˆæ£€æŸ¥é¢„å¤„ç†å‡½æ•°çš„è¾“å‡º ---
    console.print("\n[bold yellow]----------- DEBUG: Inspecting pre-processing output -----------[/bold yellow]")
    processed_text, global_refs = preprocess_history_for_report(history)
    console.print(f"Type of global_refs: {type(global_refs)}")
    console.print(f"Length of global_refs: {len(global_refs)}")
    console.print(f"Content of global_refs:\n---\n{global_refs}\n---")
    console.print("[bold yellow]-------------------- END OF DEBUG --------------------[/bold yellow]\n")


    console.print("\n[bold magenta]æŠ¥å‘Šç”Ÿæˆä¸­:[/bold magenta] ", end="")
    report_body = ""
    final_references = ""
    
    # è°ƒç”¨å·²ç»æ›´æ–°çš„æŠ¥å‘Šç”Ÿæˆæµ
    response_stream = generate_research_report_stream(history, model_name=REPORT_MODEL_FOR_TEST)
    
    for event in response_stream:
        if event["type"] == "report_chunk":
            print(event["chunk"], end="", flush=True)
            report_body += event["chunk"]
        elif event["type"] == "final_references":
            # æ”¶åˆ°ç”±ä»£ç ç”Ÿæˆçš„ã€100%å‡†ç¡®çš„å¼•ç”¨åˆ—è¡¨
            final_references = event["content"]
        elif event["type"] == "final_response":
            print() # æ¢è¡Œ
        elif event["type"] == "error":
            console.print(f"\n[bold red]é”™è¯¯:[/bold red] {event['message']}")
            report_body = event["message"]
            break
            
    # å°†æŠ¥å‘Šæ­£æ–‡å’Œå‡†ç¡®çš„å¼•ç”¨åˆ—è¡¨æ‹¼æ¥èµ·æ¥
    full_report = report_body
    if final_references:
        # ç§»é™¤å¯èƒ½ç”±æ¨¡å‹æ„å¤–ç”Ÿæˆçš„å¼•ç”¨æ ‡é¢˜
        report_body = re.sub(r"(?i)(?:\*\*References:|å‚è€ƒæ–‡çŒ®:)\s*$", "", report_body.strip())
        full_report = report_body + "\n\n## References\n" + final_references

    console.print(Panel("[bold green]âœ… æŠ¥å‘Šç”Ÿæˆå®Œæ¯•[/bold green]", expand=False))
    console.print(Markdown(full_report))
    console.print("-" * 50)


async def main_console_app():
    """
    ä¸€ä¸ªç”¨äºæµ‹è¯•æœ€ç»ˆä¼˜åŒ–æ–¹æ¡ˆçš„ç»ˆç«¯åº”ç”¨ã€‚
    """
    init_config()
    
    console.print(Markdown("# ğŸ¤– Gemini å¼•ç”¨æ’åºåŠŸèƒ½æµ‹è¯• (æœ€ç»ˆç‰ˆ)"))
    console.print(f"æœç´¢æ¨¡å‹: [cyan]{SEARCH_MODEL_FOR_TEST}[/cyan] | æŠ¥å‘Šæ¨¡å‹: [cyan]{REPORT_MODEL_FOR_TEST}[/cyan]")
    console.print("-" * 50)

    if not get_config("api_key"):
        console.print("[bold red]é”™è¯¯: æœªæ‰¾åˆ° Gemini API Keyã€‚[/bold red]")
        return

    history = []

    # --- å¯¹è¯è½®æ¬¡ ---
    prompts = [
        "ç¥ç»å†…é•œæœ‰å“ªäº›ä¸»æµçš„å‚å•†ï¼Ÿ",
        "ç¥ç»å†…é•œæœ‰ä»€ä¹ˆç”¨ï¼Ÿ"
    ]
    for prompt in prompts:
        assistant_response = await run_chat_turn(prompt, history)
        history.append({"role": "user", "content": prompt, "type": "chat"})
        history.append(assistant_response)
    
    # --- ç”ŸæˆæŠ¥å‘Š ---
    await run_report_generation(history)


if __name__ == "__main__":
    try:
        import rich
    except ImportError:
        print("Rich åº“æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install rich")
        sys.exit(1)
        
    asyncio.run(main_console_app())